{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7e6408",
   "metadata": {},
   "source": [
    "# Predicción de Ventas usando XGBoost y Características Temporales\n",
    "\n",
    "Este notebook implementa un modelo de predicción de ventas utilizando XGBoost y características temporales avanzadas. El enfoque está basado en el ejemplo de predicción de uso de bicicletas compartidas, pero adaptado al contexto de predicción de ventas en supermercados de Ecuador.\n",
    "\n",
    "## Objetivos\n",
    "1. Implementar un modelo XGBoost para predicción de ventas\n",
    "2. Crear características temporales relevantes\n",
    "3. Optimizar los hiperparámetros del modelo\n",
    "4. Evaluar el rendimiento en diferentes condiciones temporales\n",
    "\n",
    "## Contenido\n",
    "1. Carga y Preprocesamiento de Datos\n",
    "2. Ingeniería de Características\n",
    "3. Configuración del Modelo XGBoost\n",
    "4. Entrenamiento y Validación\n",
    "5. Predicción y Evaluación\n",
    "6. Validación Cruzada Temporal\n",
    "7. Optimización de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2ee68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feature_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatetimeFeatures\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CyclicalFeatures\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RollingFeatures\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'feature_engine'"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import feature_engine\n",
    "    from feature_engine.datetime import DatetimeFeatures\n",
    "    from feature_engine.creation import CyclicalFeatures\n",
    "    print(\"feature_engine imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing feature_engine: {e}\")\n",
    "\n",
    "try:\n",
    "    import skforecast\n",
    "    from skforecast.preprocessing import RollingFeatures\n",
    "    print(\"skforecast imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing skforecast: {e}\")\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Modelling and Forecasting\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import optuna\n",
    "\n",
    "# Configuración de warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "\n",
    "# Configuración de visualización\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\\nVersiones de paquetes instalados:\")\n",
    "print(f\"Versión xgboost: {xgb.__version__}\")\n",
    "print(f\"Versión numpy: {np.__version__}\")\n",
    "print(f\"Versión pandas: {pd.__version__}\")\n",
    "try:\n",
    "    print(f\"Versión feature_engine: {feature_engine.__version__}\")\n",
    "except NameError:\n",
    "    print(\"feature_engine no está instalado\")\n",
    "try:\n",
    "    print(f\"Versión skforecast: {skforecast.__version__}\")\n",
    "except NameError:\n",
    "    print(\"skforecast no está instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23392b3",
   "metadata": {},
   "source": [
    "## 1. Carga y Preprocesamiento de Datos\n",
    "\n",
    "En esta sección cargaremos los datos de ventas y realizaremos el preprocesamiento inicial:\n",
    "1. Carga de archivos CSV\n",
    "2. Conversión de fechas\n",
    "3. Manejo de valores faltantes\n",
    "4. División en conjuntos de entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ba6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df_data = pd.read_csv('data/data.csv')\n",
    "df_stores = pd.read_csv('data/stores.csv')\n",
    "df_oil = pd.read_csv('data/oil.csv')\n",
    "\n",
    "# Convertir fechas\n",
    "df_data['date'] = pd.to_datetime(df_data['date'])\n",
    "df_oil['date'] = pd.to_datetime(df_oil['date'])\n",
    "\n",
    "# Mostrar información de los datasets\n",
    "print(\"=== Dataset de Ventas ===\")\n",
    "print(df_data.info())\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "print(df_data.head())\n",
    "\n",
    "print(\"\\n=== Dataset de Tiendas ===\")\n",
    "print(df_stores.info())\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "print(df_stores.head())\n",
    "\n",
    "print(\"\\n=== Dataset de Precios del Petróleo ===\")\n",
    "print(df_oil.info())\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "print(df_oil.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaaea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionar datos de ventas con información de tiendas\n",
    "df = df_data.merge(df_stores, on='store_nbr', how='left')\n",
    "\n",
    "# Fusionar con datos de petróleo\n",
    "df = df.merge(df_oil, on='date', how='left')\n",
    "\n",
    "# Manejar valores faltantes\n",
    "df['dcoilwtico'].fillna(method='ffill', inplace=True)  # Forward fill para precios del petróleo\n",
    "\n",
    "# Crear índice temporal\n",
    "df.set_index('date', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento, validación y test\n",
    "train_end = '2016-12-31'\n",
    "valid_end = '2017-04-30'\n",
    "\n",
    "train_data = df[:'2016-12-31']\n",
    "valid_data = df['2017-01-01':'2017-04-30']\n",
    "test_data = df['2017-05-01':]\n",
    "\n",
    "print(\"=== División de datos ===\")\n",
    "print(f\"Entrenamiento: {train_data.index.min()} hasta {train_data.index.max()} (n={len(train_data)})\")\n",
    "print(f\"Validación   : {valid_data.index.min()} hasta {valid_data.index.max()} (n={len(valid_data)})\")\n",
    "print(f\"Test         : {test_data.index.min()} hasta {test_data.index.max()} (n={len(test_data)})\")\n",
    "\n",
    "# Visualizar la distribución de ventas\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df['sales'], bins=50)\n",
    "plt.title('Distribución de Ventas')\n",
    "plt.xlabel('Ventas')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas descriptivas de ventas\n",
    "print(\"\\n=== Estadísticas de Ventas ===\")\n",
    "print(df['sales'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9fa3dd",
   "metadata": {},
   "source": [
    "## 2. Ingeniería de Características\n",
    "\n",
    "Crearemos características relevantes para la predicción de ventas:\n",
    "1. Características temporales (día de la semana, mes, año, etc.)\n",
    "2. Características cíclicas\n",
    "3. Características de rezago (lag features)\n",
    "4. Características de ventana móvil (rolling features)\n",
    "5. Codificación de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear características\n",
    "def create_features(df):\n",
    "    # Características temporales\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['week_of_year'] = df.index.isocalendar().week\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Características cíclicas para mes y día de la semana\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
    "    \n",
    "    # One-hot encoding para variables categóricas\n",
    "    df = pd.get_dummies(df, columns=['type', 'city', 'state'], drop_first=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para crear características de lag y ventana móvil\n",
    "def create_lag_features(df, target='sales', lags=[7, 14, 30], windows=[7, 14, 30]):\n",
    "    group_cols = ['store_nbr', 'family']\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df.groupby(group_cols)[target].shift(lag)\n",
    "    \n",
    "    for window in windows:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby(group_cols)[target].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window).mean()\n",
    "        )\n",
    "        df[f'rolling_std_{window}'] = df.groupby(group_cols)[target].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window).std()\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar creación de características\n",
    "print(\"Creando características...\")\n",
    "df = create_features(df)\n",
    "df = create_lag_features(df)\n",
    "\n",
    "# Eliminar filas con valores NaN (primeras filas debido a los lags)\n",
    "df = df.dropna()\n",
    "\n",
    "# Mostrar las nuevas características\n",
    "print(\"\\nNuevas características creadas:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Mostrar ejemplo de datos con las nuevas características\n",
    "print(\"\\nEjemplo de datos con nuevas características:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c5173",
   "metadata": {},
   "source": [
    "## 3. Configuración del Modelo XGBoost\n",
    "\n",
    "Configuraremos el modelo XGBoost con parámetros iniciales adecuados para series temporales. \n",
    "Los parámetros clave incluyen:\n",
    "- `max_depth`: Profundidad máxima de los árboles\n",
    "- `learning_rate`: Tasa de aprendizaje\n",
    "- `n_estimators`: Número de árboles\n",
    "- `subsample`: Fracción de muestras para entrenar cada árbol\n",
    "- `colsample_bytree`: Fracción de características para cada árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y objetivo\n",
    "feature_cols = [col for col in df.columns if col not in ['sales']]\n",
    "X = df[feature_cols]\n",
    "y = df['sales']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento, validación y test manteniendo el orden temporal\n",
    "X_train = X[:'2016-12-31']\n",
    "y_train = y[:'2016-12-31']\n",
    "X_valid = X['2017-01-01':'2017-04-30']\n",
    "y_valid = y['2017-01-01':'2017-04-30']\n",
    "X_test = X['2017-05-01':]\n",
    "y_test = y['2017-05-01':]\n",
    "\n",
    "# Configurar modelo XGBoost inicial\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 1000,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 1,\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'hist',  # Para mejor rendimiento\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "\n",
    "# Crear el modelo\n",
    "model = XGBRegressor(**xgb_params)\n",
    "\n",
    "# Entrenar con early stopping\n",
    "print(\"Entrenando modelo XGBoost...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_metric=['rmse'],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Evaluar en conjunto de validación\n",
    "valid_preds = model.predict(X_valid)\n",
    "valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_preds))\n",
    "valid_mae = mean_absolute_error(y_valid, valid_preds)\n",
    "\n",
    "print(\"\\nResultados en conjunto de validación:\")\n",
    "print(f\"RMSE: {valid_rmse:.2f}\")\n",
    "print(f\"MAE: {valid_mae:.2f}\")\n",
    "\n",
    "# Visualizar importancia de características\n",
    "plt.figure(figsize=(12, 6))\n",
    "importances = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values('importance', ascending=False).head(20)\n",
    "plt.barh(importances['feature'], importances['importance'])\n",
    "plt.title('Top 20 Características más Importantes')\n",
    "plt.xlabel('Importancia')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a813dd",
   "metadata": {},
   "source": [
    "## 4. Optimización de Hiperparámetros\n",
    "\n",
    "Utilizaremos Optuna para realizar una búsqueda eficiente de los mejores hiperparámetros para nuestro modelo XGBoost. \n",
    "La optimización se centrará en los siguientes parámetros:\n",
    "- Profundidad máxima del árbol\n",
    "- Tasa de aprendizaje\n",
    "- Número de estimadores\n",
    "- Submuestreo\n",
    "- Regularización\n",
    "- Peso mínimo por hoja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir función objetivo para Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 42,\n",
    "        'tree_method': 'hist',\n",
    "        'objective': 'reg:squarederror'\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    \n",
    "    # Entrenar con early stopping\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predecir en conjunto de validación\n",
    "    valid_preds = model.predict(X_valid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, valid_preds))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Crear estudio de Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Mostrar mejores parámetros\n",
    "print(\"\\nMejores parámetros encontrados:\")\n",
    "print(study.best_params)\n",
    "print(f\"\\nMejor RMSE: {study.best_value:.2f}\")\n",
    "\n",
    "# Crear y entrenar modelo final con los mejores parámetros\n",
    "best_params = study.best_params\n",
    "best_model = XGBRegressor(**best_params, random_state=42, tree_method='hist')\n",
    "\n",
    "print(\"\\nEntrenando modelo final con los mejores parámetros...\")\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_metric=['rmse'],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Evaluar en conjunto de test\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "\n",
    "print(\"\\nResultados finales en conjunto de test:\")\n",
    "print(f\"RMSE: {test_rmse:.2f}\")\n",
    "print(f\"MAE: {test_mae:.2f}\")\n",
    "\n",
    "# Visualizar predicciones vs valores reales\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test.values, label='Real', alpha=0.5)\n",
    "plt.plot(y_test.index, test_preds, label='Predicción', alpha=0.5)\n",
    "plt.title('Predicciones vs Valores Reales en Conjunto de Test')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Ventas')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuevo_entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
